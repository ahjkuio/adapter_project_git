================================================================================
СРАВНЕНИЕ АДАПТЕРОВ LoRA И MLP
Дата: 2025-03-15 19:38:13
================================================================================

ИНФОРМАЦИЯ ОБ АДАПТЕРАХ
--------------------------------------------------------------------------------
+---------------------+----------------+--------------+
| Параметр            | LoRA           | MLP          |
+=====================+================+==============+
| Обучаемые параметры | ~2.1М          | 4211200      |
+---------------------+----------------+--------------+
| Ранг (r)            | 4              | N/A          |
+---------------------+----------------+--------------+
| Alpha (α)           | 8              | N/A          |
+---------------------+----------------+--------------+
| Скрытый размер      | N/A            | 128          |
+---------------------+----------------+--------------+
| Количество слоев    | N/A            | 4            |
+---------------------+----------------+--------------+
| Целевые модули      | q_proj, v_proj | N/A          |
+---------------------+----------------+--------------+
| Слои для адаптации  | N/A            | 0, 6, 12, 18 |
+---------------------+----------------+--------------+
| Dropout             | 0.05           | 0.1          |
+---------------------+----------------+--------------+

СРАВНЕНИЕ НА ДАТАСЕТЕ TERRa (общие способности)
--------------------------------------------------------------------------------
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+
| Метрика   |   Базовая модель |   LoRA |    MLP | LoRA vs Base     | MLP vs Base       | MLP vs LoRA       |
+===========+==================+========+========+==================+===================+===================+
| Accuracy  |           0.47   | 0.47   | 0.39   | +0.0000 (+0.00%) | -0.0800 (-17.02%) | -0.0800 (-17.02%) |
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+
| Precision |           0.5    | 0.5    | 0.5    | +0.0000 (+0.00%) | +0.0000 (+0.00%)  | +0.0000 (+0.00%)  |
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+
| Recall    |           0.235  | 0.235  | 0.195  | +0.0000 (+0.00%) | -0.0400 (-17.02%) | -0.0400 (-17.02%) |
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+
| F1        |           0.3197 | 0.3197 | 0.2806 | +0.0000 (+0.00%) | -0.0392 (-12.25%) | -0.0392 (-12.25%) |
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+

СРАВНЕНИЕ НА ДАТАСЕТЕ NERUS (специализированная задача)
--------------------------------------------------------------------------------
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+
| Метрика   |   Базовая модель |   LoRA |    MLP | LoRA vs Base     | MLP vs Base       | MLP vs LoRA       |
+===========+==================+========+========+==================+===================+===================+
| Precision |           0.3217 | 0.3217 | 0.3875 | +0.0000 (+0.00%) | +0.0658 (+20.47%) | +0.0658 (+20.47%) |
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+
| Recall    |           0.5565 | 0.5565 | 0.7062 | +0.0000 (+0.00%) | +0.1497 (+26.89%) | +0.1497 (+26.89%) |
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+
| F1        |           0.3885 | 0.3885 | 0.4726 | +0.0000 (+0.00%) | +0.0841 (+21.64%) | +0.0841 (+21.64%) |
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+

ОБЩИЕ ВЫВОДЫ
--------------------------------------------------------------------------------
1. На датасете TERRa (общие способности) лучшие результаты показал метод LoRA.
   - LoRA: изменение F1 +0.00% относительно базовой модели
   - MLP: изменение F1 -12.25% относительно базовой модели

2. На датасете Nerus (специализированная задача) лучшие результаты показал метод MLP.
   - LoRA: изменение F1 +0.00% относительно базовой модели
   - MLP: изменение F1 +21.64% относительно базовой модели

3. С точки зрения баланса между специализацией и сохранением общих способностей лучше справился метод MLP.
   - LoRA: общий баланс +0.00
   - MLP: общий баланс +9.39

РЕКОМЕНДАЦИИ:
1. Рекомендуется использовать MLP адаптеры для лучшего баланса между специализацией и общими способностями.

2. Рекомендуемые дальнейшие эксперименты:
   - Провести эксперименты с различными рангами (r) и alpha для LoRA
   - Тестирование разных размеров скрытого слоя MLP адаптеров
   - Эксперименты с комбинированным подходом, использующим оба типа адаптеров
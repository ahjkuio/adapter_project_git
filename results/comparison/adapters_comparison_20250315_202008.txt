================================================================================
СРАВНЕНИЕ АДАПТЕРОВ LoRA И MLP
Дата: 2025-03-15 20:20:08
================================================================================

ИНФОРМАЦИЯ ОБ АДАПТЕРАХ
--------------------------------------------------------------------------------
+---------------------+----------------+--------------+
| Параметр            | LoRA           | MLP          |
+=====================+================+==============+
| Обучаемые параметры | ~2.1М          | 4211200      |
+---------------------+----------------+--------------+
| Ранг (r)            | 4              | N/A          |
+---------------------+----------------+--------------+
| Alpha (α)           | 8              | N/A          |
+---------------------+----------------+--------------+
| Скрытый размер      | N/A            | 128          |
+---------------------+----------------+--------------+
| Количество слоев    | N/A            | 4            |
+---------------------+----------------+--------------+
| Целевые модули      | q_proj, v_proj | N/A          |
+---------------------+----------------+--------------+
| Слои для адаптации  | N/A            | 0, 6, 12, 18 |
+---------------------+----------------+--------------+
| Dropout             | 0.05           | 0.1          |
+---------------------+----------------+--------------+

СРАВНЕНИЕ НА ДАТАСЕТЕ TERRa (общие способности)
--------------------------------------------------------------------------------
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+
| Метрика   |   Базовая модель |   LoRA |    MLP | LoRA vs Base     | MLP vs Base       | MLP vs LoRA       |
+===========+==================+========+========+==================+===================+===================+
| Accuracy  |           0.44   | 0.44   | 0.39   | +0.0000 (+0.00%) | -0.0500 (-11.36%) | -0.0500 (-11.36%) |
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+
| Precision |           0.5    | 0.5    | 0.5    | +0.0000 (+0.00%) | +0.0000 (+0.00%)  | +0.0000 (+0.00%)  |
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+
| Recall    |           0.22   | 0.22   | 0.195  | +0.0000 (+0.00%) | -0.0250 (-11.36%) | -0.0250 (-11.36%) |
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+
| F1        |           0.3056 | 0.3056 | 0.2806 | +0.0000 (+0.00%) | -0.0250 (-8.18%)  | -0.0250 (-8.18%)  |
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+

СРАВНЕНИЕ НА ДАТАСЕТЕ NERUS (специализированная задача)
--------------------------------------------------------------------------------
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+
| Метрика   |   Базовая модель |   LoRA |    MLP | LoRA vs Base     | MLP vs Base       | MLP vs LoRA       |
+===========+==================+========+========+==================+===================+===================+
| Precision |           0.2983 | 0.2983 | 0.3875 | +0.0000 (+0.00%) | +0.0892 (+29.89%) | +0.0892 (+29.89%) |
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+
| Recall    |           0.524  | 0.524  | 0.7062 | +0.0000 (+0.00%) | +0.1822 (+34.76%) | +0.1822 (+34.76%) |
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+
| F1        |           0.3615 | 0.3615 | 0.4726 | +0.0000 (+0.00%) | +0.1111 (+30.72%) | +0.1111 (+30.72%) |
+-----------+------------------+--------+--------+------------------+-------------------+-------------------+

ОБЩИЕ ВЫВОДЫ
--------------------------------------------------------------------------------
1. На датасете TERRa (общие способности) лучшие результаты показал метод LoRA.
   - LoRA: изменение F1 +0.00% относительно базовой модели
   - MLP: изменение F1 -8.18% относительно базовой модели

2. На датасете Nerus (специализированная задача) лучшие результаты показал метод MLP.
   - LoRA: изменение F1 +0.00% относительно базовой модели
   - MLP: изменение F1 +30.72% относительно базовой модели

3. С точки зрения баланса между специализацией и сохранением общих способностей лучше справился метод MLP.
   - LoRA: общий баланс +0.00
   - MLP: общий баланс +22.55

РЕКОМЕНДАЦИИ:
1. Рекомендуется использовать MLP адаптеры для лучшего баланса между специализацией и общими способностями.

2. Рекомендуемые дальнейшие эксперименты:
   - Провести эксперименты с различными рангами (r) и alpha для LoRA
   - Тестирование разных размеров скрытого слоя MLP адаптеров
   - Эксперименты с комбинированным подходом, использующим оба типа адаптеров
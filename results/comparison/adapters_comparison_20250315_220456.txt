================================================================================
СРАВНЕНИЕ АДАПТЕРОВ LoRA И MLP
Дата: 2025-03-15 22:04:56
================================================================================

ИНФОРМАЦИЯ ОБ АДАПТЕРАХ
--------------------------------------------------------------------------------
+---------------------+----------------+--------------+
| Параметр            | LoRA           | MLP          |
+=====================+================+==============+
| Обучаемые параметры | ~2.1М          | 4211200      |
+---------------------+----------------+--------------+
| Ранг (r)            | 4              | N/A          |
+---------------------+----------------+--------------+
| Alpha (α)           | 8              | N/A          |
+---------------------+----------------+--------------+
| Скрытый размер      | N/A            | 128          |
+---------------------+----------------+--------------+
| Количество слоев    | N/A            | 4            |
+---------------------+----------------+--------------+
| Целевые модули      | q_proj, v_proj | N/A          |
+---------------------+----------------+--------------+
| Слои для адаптации  | N/A            | 0, 6, 12, 18 |
+---------------------+----------------+--------------+
| Dropout             | 0.05           | 0.1          |
+---------------------+----------------+--------------+

СРАВНЕНИЕ НА ДАТАСЕТЕ TERRa (общие способности)
--------------------------------------------------------------------------------
+-----------+------------------+--------+--------+-------------------+-------------------+-------------------+
| Метрика   |   Базовая модель |   LoRA |    MLP | LoRA vs Base      | MLP vs Base       | MLP vs LoRA       |
+===========+==================+========+========+===================+===================+===================+
| Accuracy  |           0.44   | 0.72   | 0.39   | +0.2800 (+63.64%) | -0.0500 (-11.36%) | -0.3300 (-45.83%) |
+-----------+------------------+--------+--------+-------------------+-------------------+-------------------+
| Precision |           0.5    | 0.5    | 0.5    | +0.0000 (+0.00%)  | +0.0000 (+0.00%)  | +0.0000 (+0.00%)  |
+-----------+------------------+--------+--------+-------------------+-------------------+-------------------+
| Recall    |           0.22   | 0.36   | 0.195  | +0.1400 (+63.64%) | -0.0250 (-11.36%) | -0.1650 (-45.83%) |
+-----------+------------------+--------+--------+-------------------+-------------------+-------------------+
| F1        |           0.3056 | 0.4186 | 0.2806 | +0.1130 (+37.00%) | -0.0250 (-8.18%)  | -0.1380 (-32.97%) |
+-----------+------------------+--------+--------+-------------------+-------------------+-------------------+

СРАВНЕНИЕ НА ДАТАСЕТЕ NERUS (специализированная задача)
--------------------------------------------------------------------------------
+-----------+------------------+--------+--------+-------------------+-------------------+-------------------+
| Метрика   |   Базовая модель |   LoRA |    MLP | LoRA vs Base      | MLP vs Base       | MLP vs LoRA       |
+===========+==================+========+========+===================+===================+===================+
| Precision |           0.2983 | 0.4683 | 0.3875 | +0.1700 (+56.98%) | +0.0892 (+29.89%) | -0.0808 (-17.26%) |
+-----------+------------------+--------+--------+-------------------+-------------------+-------------------+
| Recall    |           0.524  | 0.781  | 0.7062 | +0.2570 (+49.04%) | +0.1822 (+34.76%) | -0.0748 (-9.58%)  |
+-----------+------------------+--------+--------+-------------------+-------------------+-------------------+
| F1        |           0.3615 | 0.5611 | 0.4726 | +0.1995 (+55.19%) | +0.1111 (+30.72%) | -0.0885 (-15.77%) |
+-----------+------------------+--------+--------+-------------------+-------------------+-------------------+

ОБЩИЕ ВЫВОДЫ
--------------------------------------------------------------------------------
1. На датасете TERRa (общие способности) лучшие результаты показал метод LoRA.
   - LoRA: изменение F1 +37.00% относительно базовой модели
   - MLP: изменение F1 -8.18% относительно базовой модели

2. На датасете Nerus (специализированная задача) лучшие результаты показал метод LoRA.
   - LoRA: изменение F1 +55.19% относительно базовой модели
   - MLP: изменение F1 +30.72% относительно базовой модели

3. С точки зрения баланса между специализацией и сохранением общих способностей лучше справился метод LoRA.
   - LoRA: общий баланс +92.19
   - MLP: общий баланс +22.55

РЕКОМЕНДАЦИИ:
1. Рекомендуется использовать LoRA адаптеры для лучшего баланса между специализацией и общими способностями.

2. Рекомендуемые дальнейшие эксперименты:
   - Провести эксперименты с различными рангами (r) и alpha для LoRA
   - Тестирование разных размеров скрытого слоя MLP адаптеров
   - Эксперименты с комбинированным подходом, использующим оба типа адаптеров